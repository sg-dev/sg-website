<!doctype html><html lang=en><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>Massively Multilingual Natural Language Processing | Chair of Systems Design</title><link href=https://www.sg.ethz.ch/sass/eth.min.a5dc3c7d0b2e2848d52eccc0e1b6572a3cce3eeb8f524d1418d47cd85f48408c.css rel=stylesheet type=text/css><link href=https://www.sg.ethz.ch/sass/hamburgers.min.min.cabf4f2db782cf15be3c521a00e22edd7c2a1246c20739da65a5ec0967ff9f58.css rel=stylesheet type=text/css><link href=https://www.sg.ethz.ch/sass/main.min.6861d1c8f4ea48e7ba5f1b384e939d63373cdde835b1079568cc34a286751525.css rel=stylesheet type=text/css><link href=https://www.sg.ethz.ch/sass/github-markdown.min.ab4b3a69f9e4859709c1f7c8099f5a2fbce04365a13ee9c1245288405ff44a37.css rel=stylesheet type=text/css><link rel=apple-touch-icon sizes=180x180 href=https://www.sg.ethz.ch/fav/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://www.sg.ethz.ch/fav/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://www.sg.ethz.ch/fav/favicon-16x16.png><link rel=manifest href=https://www.sg.ethz.ch/fav/site.webmanifest><link rel=mask-icon href=https://www.sg.ethz.ch/fav/safari-pinned-tab.svg color=#a8322c><meta name=msapplication-TileColor content="#da532c"><meta name=theme-color content="#ffffff"><meta property="og:title" content="Massively Multilingual Natural Language Processing"><meta property="og:description" content="Abstract Multilingual language models (LMs) (e.g., multilingual BERT or XLM-R) have pushed the state of the art in multilingual NLP, yielding robust performance for various NLP tasks for languages with little or no task-specific training data. Multilingual LMs, however, suffer from a phenomenon known as curse of multilinguality: for a fixed model capacity, representations of individual languages deteriorate with inclusion of more languages into pretraining. The quality of text encodings thus varies drastically across languages, correlating highly with the size of the languages’ pretraining corpora."><meta property="og:type" content="article"><meta property="og:url" content="https://www.sg.ethz.ch/talks/2023_dh/glavas/"><meta property="article:section" content="talks"><meta name=twitter:card content="summary"><meta name=twitter:title content="Massively Multilingual Natural Language Processing"><meta name=twitter:description content="Abstract Multilingual language models (LMs) (e.g., multilingual BERT or XLM-R) have pushed the state of the art in multilingual NLP, yielding robust performance for various NLP tasks for languages with little or no task-specific training data. Multilingual LMs, however, suffer from a phenomenon known as curse of multilinguality: for a fixed model capacity, representations of individual languages deteriorate with inclusion of more languages into pretraining. The quality of text encodings thus varies drastically across languages, correlating highly with the size of the languages’ pretraining corpora."></head><body><div class=site-wrapper><header class="site-header header--with-department-breadcrumb"><div class="header__nav-meta header__desktop-only"><div class=header__container><nav class=header__departments><h2 class=hidden>Departments</h2><ul><li class=item--organization><a href=https://ethz.ch/en.html>ETH Zurich</a></li><li class=item--organization><a href=https://mtec.ethz.ch/>D-MTEC</a></li><li class=item--organization><a href=https://www.sg.ethz.ch/>SG</a></li></ul></nav></div></div><div class="header__logos header__container"><a href=https://ethz.ch/en.html><img src=https://www.sg.ethz.ch/img/ethz_logo_black.svg alt="Logo of ETH Zurich, to homepage"></a><div class=header__headlines><h2 class=header__headline--big><a href=https://www.sg.ethz.ch/>Chair of Systems Design</a></h2></div></div><div class=header__nav-primary><div class="header__navbar header__container"><button class="hamburger hamburger--collapse" type=button>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><nav class="nav-primary js-nav-primary"><div class="header__nav-level header__nav-level--root js-header__nav-level--root" role=navigation><h1 class=screenreader>Navigation Area</h1><ul id=navList><li><a class=nav-link href=https://www.sg.ethz.ch/>Home</a></li><li><a class=nav-link href=https://www.sg.ethz.ch/publications/>Publications</a></li><li><a class=nav-link href=https://www.sg.ethz.ch/team/>Team</a></li><li><a class=nav-link href=https://www.sg.ethz.ch/projects/>Projects</a></li><li><a class=nav-link href=https://www.sg.ethz.ch/courses/>Courses</a></li><li><a class=nav-link href=https://www.sg.ethz.ch/news/>News</a></li><li><a class=nav-link href=https://www.sg.ethz.ch/about/>About Us</a></li></ul></div></nav></div></div></header><section id=content class=site-content><div><main id=main><section class="section section__top section__bg section__bgprimary"><div class=section__bgprimary__image style="background:url(/events/digital-humanities/bg.png)no-repeat 50%;background-size:cover"></div><div class="section__inner section__content"><div class="section__top__text talk"><h2><a class=event-tag href=https://www.sg.ethz.ch/events/digital-humanities>Digital Humanities</a></h2><h1>Massively Multilingual Natural Language Processing</h1><h2><strong>Goran Glavaš</strong><br><i>JMU Würzburg</i></h2><p></p><h2>7 Feb 2023, 14:45–15:30<br><strong></strong></h2></div></div></section><section class="section section__bg section__bglight wave-deactivate"><div class="section__inner section__content"><div class="text markdown-body"><div class=imgcontainer><img src=https://www.sg.ethz.ch/talks/2023_dh/glavas/profile_pic_huaf9c98d6a7f26259f8837f22ed97a2fd_59984_300x300_fill_q75_box_center.jpg></div><h3 id=abstract>Abstract</h3><p>Multilingual language models (LMs) (e.g., multilingual BERT or XLM-R) have pushed the state of the art in multilingual NLP, yielding robust performance for various NLP tasks for languages with little or no task-specific training data. Multilingual LMs, however, suffer from a phenomenon known as curse of multilinguality: for a fixed model capacity, representations of individual languages deteriorate with inclusion of more languages into pretraining. The quality of text encodings thus varies drastically across languages, correlating highly with the size of the languages’ pretraining corpora. I will present work on remedying the curse of multilinguality and improving NLP models for low-resource languages, including the approaches that leverage massively multilingual lexical resources (e.g., BabelNet, PanLex).</p></div></div></section></main></div></section><footer id=footer class="site-footer section__bg section__bgdark wave-deactivate"><div class=footer__container><nav aria-labelledby=footer-resources-heading class="footer__row footer__resources"><h3 id=footer-resources-heading class=visually-hidden>Resources</h3><ul><li><a href=https://www.sg.ethz.ch/admin/privacy/>Data Protection</a></li><li><a href=https://www.sg.ethz.ch/admin/disclaimer/>Disclaimer</a></li><li><a href=https://www.sg.ethz.ch/admin/imprint/>Imprint</a></li><li><a href=https://www.sg.ethz.ch/admin/contactus/>Contact Us</a></li></ul></nav><div class=footer__copyright>© 2023 Chair of Systems Design | ETH Zürich</a></div></div></footer><script src=https://www.sg.ethz.ch/js/timeago.min.js></script>
<script src=https://www.sg.ethz.ch/js/jquery-3.5.1.min.js></script>
<script src=https://www.sg.ethz.ch/js/executes.js></script></div></body></html>